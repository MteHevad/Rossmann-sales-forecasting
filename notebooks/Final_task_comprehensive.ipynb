{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sda7b7tQzHu"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing\n",
        "data_path = r'C:\\Users\\hp\\Desktop\\KAIM\\Week 4\\rossmann-store-sales\\\\'"
      ],
      "metadata": {
        "id": "djJ7ZM7BQ41H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train = pd.read_csv(data_path + 'train.csv', dtype={'StateHoliday': str}, low_memory=False)\n",
        "store = pd.read_csv(data_path + 'store.csv')\n",
        "test = pd.read_csv(data_path + 'test.csv', dtype={'StateHoliday': str}, low_memory=False)"
      ],
      "metadata": {
        "id": "7iF7DZufQ6iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge store data with train and test datasets\n",
        "train = pd.merge(train, store, on='Store', how='left')\n",
        "test = pd.merge(test, store, on='Store', how='left')"
      ],
      "metadata": {
        "id": "Hu57cSWaQ8HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values\n",
        "train['CompetitionDistance'] = train['CompetitionDistance'].fillna(train['CompetitionDistance'].median())\n",
        "train['Promo2SinceYear'] = train['Promo2SinceYear'].fillna(0)\n",
        "train['Promo2SinceWeek'] = train['Promo2SinceWeek'].fillna(0)\n",
        "train['PromoInterval'] = train['PromoInterval'].fillna('None')\n",
        "\n",
        "test['CompetitionDistance'] = test['CompetitionDistance'].fillna(test['CompetitionDistance'].median())\n",
        "test['Promo2SinceYear'] = test['Promo2SinceYear'].fillna(0)\n",
        "test['Promo2SinceWeek'] = test['Promo2SinceWeek'].fillna(0)\n",
        "test['PromoInterval'] = test['PromoInterval'].fillna('None')"
      ],
      "metadata": {
        "id": "nL8iE88DQ9lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Engineering - Extracting date-related features from 'Date' column\n",
        "# Convert 'Date' to datetime format and extract features\n",
        "train['Date'] = pd.to_datetime(train['Date'])\n",
        "train['Year'] = train['Date'].dt.year\n",
        "train['Month'] = train['Date'].dt.month\n",
        "train['Day'] = train['Date'].dt.day\n",
        "train['DayOfWeek'] = train['Date'].dt.dayofweek\n",
        "train['Weekday'] = train['DayOfWeek'].apply(lambda x: 1 if x < 5 else 0)\n",
        "\n",
        "test['Date'] = pd.to_datetime(test['Date'])\n",
        "test['Year'] = test['Date'].dt.year\n",
        "test['Month'] = test['Date'].dt.month\n",
        "test['Day'] = test['Date'].dt.day\n",
        "test['DayOfWeek'] = test['Date'].dt.dayofweek\n",
        "test['Weekday'] = test['DayOfWeek'].apply(lambda x: 1 if x < 5 else 0)"
      ],
      "metadata": {
        "id": "gQVK9bOhRAVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Label encoding for categorical variables\n",
        "# Combine unique labels from both train and test sets for encoding\n",
        "label_encoder = LabelEncoder()"
      ],
      "metadata": {
        "id": "VKXDorumRCT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine train and test 'StateHoliday' values to fit the encoder\n",
        "all_state_holidays = np.concatenate([train['StateHoliday'], test['StateHoliday']])\n",
        "label_encoder.fit(all_state_holidays)"
      ],
      "metadata": {
        "id": "aPxWMXNJREj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the fitted encoder to both datasets\n",
        "train['StateHoliday'] = label_encoder.transform(train['StateHoliday'])\n",
        "test['StateHoliday'] = label_encoder.transform(test['StateHoliday'])"
      ],
      "metadata": {
        "id": "ViEdEFUjRG3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding other categorical variables in both train and test sets\n",
        "train['StoreType'] = label_encoder.fit_transform(train['StoreType'])\n",
        "test['StoreType'] = label_encoder.transform(test['StoreType'])\n",
        "\n",
        "train['Assortment'] = label_encoder.fit_transform(train['Assortment'])\n",
        "test['Assortment'] = label_encoder.transform(test['Assortment'])"
      ],
      "metadata": {
        "id": "Upi9sBp4RIiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Feature Selection\n",
        "features = ['Store', 'Promo', 'StateHoliday', 'SchoolHoliday', 'DayOfWeek', 'Month', 'CompetitionDistance', 'StoreType', 'Assortment']\n",
        "X = train[features]\n",
        "y = train['Sales']"
      ],
      "metadata": {
        "id": "IM_xQ_CwRKc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bWHRNSJaRQ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to NumPy arrays to avoid feature name mismatch warnings\n",
        "X_train_np = X_train.to_numpy()\n",
        "X_val_np = X_val.to_numpy()"
      ],
      "metadata": {
        "id": "BiM5ThZ3RStm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create Pipeline and Model Training\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Scaling step\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))  # Random Forest Model\n",
        "])"
      ],
      "metadata": {
        "id": "3sAkmI-ARUwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline to the training data (NumPy arrays)\n",
        "pipeline.fit(X_train_np, y_train)"
      ],
      "metadata": {
        "id": "NhZG2huIRWnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (using NumPy array for X_val)\n",
        "y_pred = pipeline.predict(X_val_np)"
      ],
      "metadata": {
        "id": "KvXqdUKQRYyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ],
      "metadata": {
        "id": "E3FuyWn-RaQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Post-Prediction Analysis\n",
        "# Feature importance\n",
        "importances = pipeline.named_steps['rf'].feature_importances_"
      ],
      "metadata": {
        "id": "09zmDxNrRcAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display feature importance results in a readable format\n",
        "print(\"\\n--- Feature Importance ---\")\n",
        "for i, feature in enumerate(features):\n",
        "    print(f\"Feature: {feature}, Importance: {importances[i]:.4f}\")"
      ],
      "metadata": {
        "id": "CgeuhTAuRd_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence intervals estimation\n",
        "predictions_std = np.std([tree.predict(X_val_np) for tree in pipeline.named_steps['rf'].estimators_], axis=0)\n",
        "confidence_intervals = 1.96 * predictions_std"
      ],
      "metadata": {
        "id": "mKPeXYqJRf22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display predictions with confidence intervals\n",
        "print(\"\\n--- Predictions with Confidence Intervals ---\")\n",
        "for i in range(5):\n",
        "    print(f\"Prediction: {y_pred[i]:.2f}, Confidence Interval: +/- {confidence_intervals[i]:.2f}\")"
      ],
      "metadata": {
        "id": "UBn9CUUVRhlQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
